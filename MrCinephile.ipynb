{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a2e2ed",
   "metadata": {},
   "source": [
    "# 1. Import and read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebea07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies loaded: 100 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1982</td>\n",
       "      <td>Parasite</td>\n",
       "      <td>American</td>\n",
       "      <td>Charles Band</td>\n",
       "      <td>Demi Moore, Robert Glaudini</td>\n",
       "      <td>horror</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Parasite_(film)</td>\n",
       "      <td>In the near future, an atomic disaster has red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982</td>\n",
       "      <td>Partners</td>\n",
       "      <td>American</td>\n",
       "      <td>James Burrows</td>\n",
       "      <td>John Hurt, Ryan O'Neal</td>\n",
       "      <td>comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Partners_(1982_f...</td>\n",
       "      <td>After a series of murders in Los Angeles's gay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1982</td>\n",
       "      <td>Personal Best</td>\n",
       "      <td>American</td>\n",
       "      <td>Robert Towne</td>\n",
       "      <td>Mariel Hemingway, Scott Glenn, Patrice Donnelly</td>\n",
       "      <td>drama, sports</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Personal_Best_(f...</td>\n",
       "      <td>Chris Cahill is a young athlete who competes u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1982</td>\n",
       "      <td>Poltergeist</td>\n",
       "      <td>American</td>\n",
       "      <td>Tobe Hooper</td>\n",
       "      <td>Craig T. Nelson, Beatrice Straight, Dominique ...</td>\n",
       "      <td>horror</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Poltergeist_(198...</td>\n",
       "      <td>Steven and Diane Freeling live a quiet life in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1982</td>\n",
       "      <td>Porky's</td>\n",
       "      <td>American</td>\n",
       "      <td>Bob Clark</td>\n",
       "      <td>Kim Cattrall, Mark Herrier, Wyatt Knight</td>\n",
       "      <td>comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Porky%27s</td>\n",
       "      <td>A group of Florida high school students plan o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year          Title Origin/Ethnicity       Director  \\\n",
       "0          1982       Parasite         American   Charles Band   \n",
       "1          1982       Partners         American  James Burrows   \n",
       "2          1982  Personal Best         American   Robert Towne   \n",
       "3          1982    Poltergeist         American    Tobe Hooper   \n",
       "4          1982        Porky's         American      Bob Clark   \n",
       "\n",
       "                                                Cast          Genre  \\\n",
       "0                        Demi Moore, Robert Glaudini         horror   \n",
       "1                             John Hurt, Ryan O'Neal         comedy   \n",
       "2    Mariel Hemingway, Scott Glenn, Patrice Donnelly  drama, sports   \n",
       "3  Craig T. Nelson, Beatrice Straight, Dominique ...         horror   \n",
       "4           Kim Cattrall, Mark Herrier, Wyatt Knight         comedy   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0      https://en.wikipedia.org/wiki/Parasite_(film)   \n",
       "1  https://en.wikipedia.org/wiki/Partners_(1982_f...   \n",
       "2  https://en.wikipedia.org/wiki/Personal_Best_(f...   \n",
       "3  https://en.wikipedia.org/wiki/Poltergeist_(198...   \n",
       "4            https://en.wikipedia.org/wiki/Porky%27s   \n",
       "\n",
       "                                                Plot  \n",
       "0  In the near future, an atomic disaster has red...  \n",
       "1  After a series of murders in Los Angeles's gay...  \n",
       "2  Chris Cahill is a young athlete who competes u...  \n",
       "3  Steven and Diane Freeling live a quiet life in...  \n",
       "4  A group of Florida high school students plan o...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "# Read in IMDb and Wikipedia movie data (both in same file)\n",
    "movies_df = pd.read_csv('datasets/wiki_movie_plots_deduped.csv', skiprows=range(1, 10000), nrows=100)\n",
    "\n",
    "print(\"Number of movies loaded: %s \" % (len(movies_df)))\n",
    "\n",
    "# Display the data\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d1f98",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20928e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Antarctica',\n",
       " 'in',\n",
       " 'a',\n",
       " 'helicopter',\n",
       " 'pursues',\n",
       " 'a',\n",
       " 'sled',\n",
       " 'dog',\n",
       " 'to',\n",
       " 'an',\n",
       " 'American',\n",
       " 'research',\n",
       " 'station',\n",
       " 'firing',\n",
       " 'at',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'and',\n",
       " 'dropping',\n",
       " 'grenades',\n",
       " 'without',\n",
       " 'success']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "user_input = \"In Antarctica in 1982, a helicopter pursues a sled dog to an American research station, firing at the dog and dropping grenades without success. The helicopter lands and the researchers witness one of the two occupants accidentally blow up the helicopter and himself with a grenade. The remaining man shoots at the dog and shouts at the Americans in Norwegian, but they are unable to understand him. He is shot dead in self-defense by station commander Garry. The American helicopter pilot, R.J. MacReady, and Dr. Copper leave to investigate the Norwegian base. Among the charred ruins and frozen corpses, they find the burnt corpse of a malformed humanoid, which they transfer to the American station. Their biologist, Blair, performs an autopsy on the remains and finds a normal set of human organs.\"\n",
    "\n",
    "# Tokenize a paragraph into sentences and store in sent_tokenized\n",
    "sent_tokenized = [sent for sent in nltk.sent_tokenize(user_input)]\n",
    "\n",
    "# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\n",
    "words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n",
    "\n",
    "# Remove tokens that do not contain any letters from words_tokenized\n",
    "import re\n",
    "\n",
    "filtered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n",
    "\n",
    "# Display filtered words to observe words after tokenization\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115725b4",
   "metadata": {},
   "source": [
    "# 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0531bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stemming:  ['In', 'Antarctica', 'in', 'a', 'helicopter', 'pursues', 'a', 'sled', 'dog', 'to', 'an', 'American', 'research', 'station', 'firing', 'at', 'the', 'dog', 'and', 'dropping', 'grenades', 'without', 'success']\n",
      "After stemming:    ['in', 'antarctica', 'in', 'a', 'helicopt', 'pursu', 'a', 'sled', 'dog', 'to', 'an', 'american', 'research', 'station', 'fire', 'at', 'the', 'dog', 'and', 'drop', 'grenad', 'without', 'success']\n"
     ]
    }
   ],
   "source": [
    "# Import the SnowballStemmer to perform stemming\n",
    "# ... YOUR CODE FOR TASK 4 ...\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Create an English language SnowballStemmer object\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Print filtered to observe words without stemming\n",
    "print(\"Without stemming: \", filtered)\n",
    "\n",
    "# Stem the words from filtered and store in stemmed_words\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered]\n",
    "\n",
    "# Print the stemmed_words to observe words after stemming\n",
    "print(\"After stemming:   \", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5d9bd",
   "metadata": {},
   "source": [
    "# 4. Tokenize and Stem together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5a37c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'antarctica', 'in', 'a', 'helicopt', 'pursu', 'a', 'sled', 'dog', 'to', 'an', 'american', 'research', 'station', 'fire', 'at', 'the', 'dog', 'and', 'drop', 'grenad', 'without', 'success', 'the', 'helicopt', 'land', 'and', 'the', 'research', 'wit', 'one', 'of', 'the', 'two', 'occup', 'accident', 'blow', 'up', 'the', 'helicopt', 'and', 'himself', 'with', 'a', 'grenad', 'the', 'remain', 'man', 'shoot', 'at', 'the', 'dog', 'and', 'shout', 'at', 'the', 'american', 'in', 'norwegian', 'but', 'they', 'are', 'unabl', 'to', 'understand', 'him', 'he', 'is', 'shot', 'dead', 'in', 'self-defens', 'by', 'station', 'command', 'garri', 'the', 'american', 'helicopt', 'pilot', 'r.j.', 'macreadi', 'and', 'dr.', 'copper', 'leav', 'to', 'investig', 'the', 'norwegian', 'base', 'among', 'the', 'char', 'ruin', 'and', 'frozen', 'corps', 'they', 'find', 'the', 'burnt', 'corps', 'of', 'a', 'malform', 'humanoid', 'which', 'they', 'transfer', 'to', 'the', 'american', 'station', 'their', 'biologist', 'blair', 'perform', 'an', 'autopsi', 'on', 'the', 'remain', 'and', 'find', 'a', 'normal', 'set', 'of', 'human', 'organ']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to perform both stemming and tokenization\n",
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    \n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    \n",
    "    return stems\n",
    "\n",
    "user_input_copy = \"\"\"In Antarctica in 1982, a helicopter pursues a sled dog to an American research station, firing at the dog and \n",
    "dropping grenades without success. The helicopter lands and the researchers witness one of the two occupants accidentally blow \n",
    "up the helicopter and himself with a grenade. The remaining man shoots at the dog and shouts at the Americans in Norwegian, \n",
    "but they are unable to understand him. He is shot dead in self-defense by station commander Garry. The American helicopter \n",
    "pilot, R.J. MacReady, and Dr. Copper leave to investigate the Norwegian base. Among the charred ruins and frozen corpses, \n",
    "they find the burnt corpse of a malformed humanoid, which they transfer to the American station. Their biologist, Blair, \n",
    "performs an autopsy on the remains and finds a normal set of human organs.\"\"\"\n",
    "\n",
    "words_stemmed = tokenize_and_stem(user_input)\n",
    "print(words_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387f351",
   "metadata": {},
   "source": [
    "# 5. Create TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09f2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer to create TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a77a41",
   "metadata": {},
   "source": [
    "# 6. Fit transform TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd03fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 89)\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the tfidf_vectorizer with the \"plot\" of each movie\n",
    "# to create a vector representation of the plot summaries\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"Plot\"]])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca45dc",
   "metadata": {},
   "source": [
    "# 7. Import KMeans and create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4969fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38210e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
